{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM7i23QtNHY259NzWp7ZE9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Computer Vision"
      ],
      "metadata": {
        "id": "uniGCZEC6JkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Computer vision libraries\n",
        "\n",
        "* `torchvision` - Base domain library for computer vision\n",
        "* `torchvision.datasets` - get datasets and data loading functions for computer vision\n",
        "* `torchvision.models` - get pretrained computer models that we can use leverage for our own problems\n",
        "* `torchvision.transforms` - functions for manipulating vision data to be suitable for use with ML models\n",
        "* `torch.utils.data.Dataset` - Base dataset class for PyTorch\n",
        "* `torch.utils.data.Dataloader` - Creates a pyhton iterable over a dataset"
      ],
      "metadata": {
        "id": "gWPvsyuyDD0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libs\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "RKLBe7g8EBgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting a dataset\n",
        "\n",
        "FashionMNIST - Dataset we are using from torchvison.dataset"
      ],
      "metadata": {
        "id": "hHS7DYf4E44s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training data\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data\n",
        "    train=True, # do we want testing or training data?\n",
        "    download=True, # do we want to download yes/no?\n",
        "    transform=torchvision.transforms.ToTensor(), # how do we want to transform the data?\n",
        "    target_transform=None # How do we want to transform the labels/targets?\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    target_transform=None\n",
        ")"
      ],
      "metadata": {
        "id": "t8p6TlmlKfs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "ZSxYCkNLLtpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]"
      ],
      "metadata": {
        "id": "Y8uOF-iIL87W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape, type(label)"
      ],
      "metadata": {
        "id": "09tWd960MEMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "Gla6Sa8rMpWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "mgBTlLxdM46N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "CpXTnVo8NAwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Image shape: {image.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Image lable: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "2t4D51ubNGlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 visualize our data"
      ],
      "metadata": {
        "id": "9JM_Ae31Ql7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image.squeeze())\n",
        "plt.title(label)"
      ],
      "metadata": {
        "id": "Q1YkkZuDQ6pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)"
      ],
      "metadata": {
        "id": "1zfOVy8GRN5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more random images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows * cols + 1):\n",
        "  random_idx = torch.randint(0, len(train_data), size = [1]).item()\n",
        "  img, label = train_data[random_idx]\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "  plt.imshow(img.squeeze(), cmap = \"gray\")\n",
        "  plt.title(class_names[label])\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "oKmleZp-Rl4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "id": "u9doKtzm_OHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare Dataloader\n",
        "\n",
        "Our data is in form of PyTorch Dataset\n",
        "\n",
        "DataLoader turns our dataset into python iterable\n",
        "\n",
        "More specifically, we want to turn our data into batches\n",
        "\n",
        "Why?\n",
        "1. It is more computationally efficient. Computing hardware may not be able to look at all the data (60000 in our case) in 1 hit. So we break it into batches (32 most common)\n",
        "2. It gives our NN more chances to update its gradients weights and biases per epoch."
      ],
      "metadata": {
        "id": "s_L6eq8VS-j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# turn data into dataloader\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "6V4DOAxY_eNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout whats we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "KnLaLV9eD-ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out whats inside a dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "K0IvpJbyE9UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a sample of batch\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ],
      "metadata": {
        "id": "8q2WW3UtEoth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model 0: Baseline model\n",
        "\n",
        "When starting to build a series of ML modelling experiments, it's best practice to start with a baseline model\n",
        "\n",
        "A baseline model is a simple model which you try and improve upon subsequent models/experiemnts.\n",
        "\n",
        "In other words: start simple and add complexity when needed"
      ],
      "metadata": {
        "id": "-dShW5xvyl-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [colr_channels, height, width]\")\n",
        "print(f\"Shape after flatteing: {output.shape} -> [colr_channels, height*width]\")"
      ],
      "metadata": {
        "id": "3sbuqj-R0c-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               output_shape: int,\n",
        "               hidden_units: int) -> None:\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "U49Tet471K44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Setup model with i\\p parameters\n",
        "model_0 = FashionMNISTModelV0(\n",
        "    input_shape=784, # 28 * 28\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)\n",
        ").to(\"cpu\")\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "id": "rz5UvIGl1Svi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1, 1, 28, 28])\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "id": "DHjS2esw3FQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "UGCGtlmv_o7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss Function - `nn.CrossEntropyLoss`\n",
        "* Optimizer - `torch.optim.SGD()`\n",
        "* Evaluation metric - Accuracy"
      ],
      "metadata": {
        "id": "Z7YpvVVD3OsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "fEX1yL4mAnHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss, Optimizer and Evaluation\n",
        "from helper_functions import accuracy_fn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_0.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "4VmM9xiXAUuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating a Fn to time our experiments\n",
        "\n",
        "ML is very experimental\n",
        "\n",
        "2 things which we want to track are:\n",
        "1. Model's performance\n",
        "2. How fast it runs"
      ],
      "metadata": {
        "id": "-aR_nLzTBh_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float,\n",
        "                     end:float,\n",
        "                     device: torch.device = None):\n",
        "  \"\"\"Prints difference b/w start and end time\"\"\"\n",
        "  total_time = end - start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "vgHTqW8AByHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "# some code\n",
        "end_time = timer()\n",
        "print_train_time(start_time, end_time, \"cpu\")"
      ],
      "metadata": {
        "id": "8987WoW0Cg_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Creating a training loop and training a model on batches of data\n",
        "\n",
        "optimizer will update the model's parameter once per batch instead of epoch\n",
        "\n",
        "1. Loop through epochs\n",
        "2. Loop through training batch, perform training steps, calculate the train loss *per batch*\n",
        "3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*\n",
        "4. Print whats happening\n"
      ],
      "metadata": {
        "id": "79sO3lP6C5Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Select epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n\")\n",
        "  ### Training\n",
        "  train_loss = 0\n",
        "  # Add a loop to loop through training batches\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    model_0.train()\n",
        "    # 1. Forward Pass\n",
        "    y_pred = model_0(X).squeeze()\n",
        "\n",
        "    # 2. Loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print out whats happening\n",
        "    if batch % 400 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.\") # length of train_dataloader.dataset is total no of samples in the dataset\n",
        "\n",
        "  # Divide total train loss by len of train data loader\n",
        "  train_loss /= len(train_dataloader) # length of train_dataloader is number of batches\n",
        "\n",
        "  ### Testing\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in test_dataloader:\n",
        "      # 1. Forward pass\n",
        "      test_pred = model_0(X_test)\n",
        "\n",
        "      # 2. Calculate loss & acc\n",
        "      test_loss += loss_fn(test_pred, y_test)\n",
        "      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Calculate the test loss avg and acc avg\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "  # Print out what's happening\n",
        "  print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                           device=str(next(model_0.parameters()).device))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_3R9-TIvCwWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Making predictions and get model 0 results"
      ],
      "metadata": {
        "id": "YzY2gbuHEhHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "  \"\"\"Returns a dictionary containing the results of model predicting on data_loader\n",
        "\n",
        "  Args:\n",
        "      model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "      data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "      loss_fn (torch.nn.Module): The loss function of model.\n",
        "      accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "\n",
        "  Returns:\n",
        "      (dict): Results of model making predictions on data_loader.\n",
        "  \"\"\"\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Accumulate the loss and acc values per batch\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y,\n",
        "                         y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # Scale loss and acc to find avg loss/avg per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "  return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "          \"model_loss\": loss.item(),\n",
        "          \"model_acc\": acc}\n",
        "\n",
        "# Calculate model_0 results on test dataset\n",
        "model_0_results = eval_model(model=model_0,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn)"
      ],
      "metadata": {
        "id": "6o9InB5CHqx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbf667cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "105b7ace-e15c-43c8-ab05-eb3c528cd252"
      },
      "source": [
        "model_0_results"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModelV0',\n",
              " 'model_loss': 0.44271987676620483,\n",
              " 'model_acc': 84.76437699680511}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibivVFoHhcpP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}